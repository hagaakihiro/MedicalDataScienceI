<h1 id="最小二乗法と最尤推定">最小二乗法と最尤推定</h1>
<h2 id="はじめに">はじめに</h2>
<p>機械学習や人工知能を学ぶ上で、実現したいことに対して手法を分けて 整理しておくと理解が進むことがある。 基本的に、我々が機械学習や人工知能を使って実現したいことは 「回帰・識別・クラスタリング」である（図2-1） （この他に将棋ソフトのような「強化学習」というものもある。 これは勝利に導いた「過程」に報酬を与えることで何度も対戦を繰り返すことで 報酬が最大になる経路を学習していくというプロセスを踏んでいる。 「報酬の最大化」という点を見ると、これから行う回帰や識別・クラスタリング と同じような問題設定に帰結させていることに気づくであろう）。 <strong>回帰（Regression）</strong> は量的データ（数値であり、 値の大小が意味をなすもの）を再現することが目的であり、 関数フィッティング（または単にフィッティング）のことである。 <strong>識別（classification</strong> は量的データ[*]を情報として質的データ （男女、犬猫、のように大小の意味を持たないもの）を分ける境界を引くことに対応し、 <strong>クラスタリング（Clustering）</strong> は識別に似ているが、 識別のように分けたい質的データを最初から与えるのではなく、 量的データから似たもの同士をグループ化する（クラスタリングする）ことを行う。 識別は、量的データを入力として質的データを学習するが、 クラスタリングは、質的データの学習が必要ないため、 前者を<strong>教師あり学習</strong>、後者を<strong>教師なし学習</strong>として区別している。</p>
<p>**質的データと量的データについては医学統計学で学ぶ*</p>
<figure>
<img src="images/2_1.png" alt="preview" /><figcaption aria-hidden="true">preview</figcaption>
</figure>
<p><em>図2-1:機械学習・人工知能で実現させること：回帰・識別・クラスタリング</em></p>
<p>医用データ科学Iでは、前半で回帰、後半で識別を学ぶ。 回帰は先に述べた通り、数値データをフィッティングすることである。最も簡単な例は、一次元データを直線でフィッティングすることであり、 全く同じ枠組みで曲線でフィッティングしたり、多次元データに当てはめることが可能である（図2-2）。 これらが同じ枠組みで理解されるということが実感できた時、回帰がマスターできたと言えるとともに、どのようなデータに対しても立ち向かえることができる武器を手に入れたことになる。 本稿では、まず最小二乗法によってなぜ一次元データをフィッティングすることができるのかを学び、 この最小二乗法が俯瞰的に見ると、データにガウス型ノイズが付加されていると考えた時の最尤推定によって導かれることを学ぶ。</p>
<figure>
<img src="images/2_2.png" alt="preview" /><figcaption aria-hidden="true">preview</figcaption>
</figure>
<p><em>図2-2:左：直線フィッティング、中央：曲線フィッティング、右：二次元データのフィッティング。どれも回帰という機械学習の枠組みで実現される。</em></p>
<h2 id="最小二乗法">最小二乗法</h2>
<p>図2-3に示す赤の丸点を直線でフィッティングすることを考えよう。 これは横軸をCT値[Hunsfield Unit], 縦軸を水に対する相対密度としたCT値密度変換データである。 直線でフィッティングすることで、任意のCT値を相対密度に変換することができる。 赤のデータ点を<span class="math inline">(<em>x</em><sub><em>n</em></sub>, <em>t</em><sub><em>n</em></sub>)</span>と記述することにしよう。ここで<span class="math inline"><em>x</em><sub><em>n</em></sub></span>がデータ点のCT値、<span class="math inline"><em>t</em><sub><em>n</em></sub></span>が対応するデータ点の相対密度である。 今CT値を<span class="math inline"><em>x</em></span>としたときに相対密度を与える直線として<span class="math inline"><em>y</em> = <em>a</em><em>x</em> + <em>b</em></span>を仮定する。 データフィッティングの問題は、「データを再現する関数<span class="math inline"><em>y</em> = <em>a</em><em>x</em> + <em>b</em></span>の<span class="math inline"><em>a</em>, <em>b</em></span>を求める」という問題と見做せる。 「データを再現する」ということを数式上で表現するには、それを「データ点と直線との距離が最も短くなる」と読み替えることで、</p>
<p><span class="math display">$${\cal L} = \frac{1}{2}\sum_n^N (y(x_n)-t_n)^2 = \frac{1}{2}\sum_n^N ((ax_n+b)-t_n)^2 \hspace{2mm}\cdots(1)$$</span></p>
<p>を最小にする<span class="math inline"><em>a</em>, <em>b</em></span>を求めれば良いのではないか、と考えることができよう（1/2は偏微分する際に消えるので便利のためつけている）。 この式はデータ点と直線の二乗和であり、これを最小にしようとするため、最小二乗法という名前が付けられている。</p>
<!---![preview](images/2_3.png)--->
<p><img src="images/2_3.png" width="500"><br> <em>図2-3:CT値密度変換データ：横軸はCT値[Hunsfield Unit], 縦軸は水に対する相対密度。 このグラフから、ある人に頭部のCTを撮影した時の脳幹の密度を推定するにはどうすれば良いだろうか？</em></p>
<p><strong>練習１:</strong> 式（1）を<span class="math inline"><em>a</em></span>と<span class="math inline"><em>b</em></span>で偏微分した結果を示せ。 <br> <br> <br> <br> <br> <br> <br></p>
<p><strong>練習2:</strong> この偏微分の結果を用いて図2-4の三点のデータに対する直線フィッティングの式<span class="math inline"><em>y</em> = <em>a</em><em>x</em> + <em>b</em></span>を求めよ。</p>
<!---![preview](images/2_4.png)--->
<p><img src="images/2_4.png" width="400"><br> <em>図2-4:3点との差が最も小さくなる直線を求めよ（応用数学Iでやってますよ！）</em> <br> <br> <br> <br> <br> <br> <br></p>
<p>ここまでは直線フィッティングを考えてきたが、それはフィッティングの関数を直線と仮定したからであり、 別にどのような関数を用意しても最小二乗法を行うことができる。例えば多項式、</p>
<p><span class="math display">$$
y(x) = \sum_{i=0}^{M}w_i x^i = w_0 + w_1 x + w_2 x^2 \cdots + w_M x^M \hspace{2mm}\cdots(2)
$$</span> でフィッティングすることを考えてみよう。これを式(1)に代入して求めたい<span class="math inline"><em>w</em><sub><em>i</em></sub></span>で偏微分し、それが0となる<span class="math inline"><em>w</em><sub><em>i</em></sub></span>を求めれば良いわけである。 実際にやってみよう。今後のことも考えて上式を、</p>
<p><span class="math display">$$
y(x) = \sum_{i=0}^{M}w_i \phi_i(x) = w_0 + w_1 \phi_1(x) + w_2 \phi_2(x) \cdots + w_M \phi_M(x) = \vec{w}^{\top}\vec{\phi}(x) \hspace{2mm}\cdots(3)
$$</span> としておく。つまり<span class="math inline"><em>ϕ</em><sub><em>i</em></sub> = <em>x</em><sup><em>i</em></sup></span>とすれば多項式になるので、(3)は(2)を一般化したことになる。</p>
<p>これで二乗和誤差関数</p>
<p><span class="math display">$$
{\cal L}(\vec{w}) = \frac{1}{2}\sum_{n=1}^N \left( y(x_n) - t_n \right)^2 \hspace{2mm}\cdots(4)
$$</span> を<span class="math inline"><em>w</em><sub><em>i</em></sub></span>で偏微分し、0とおいて整理すると次の解析解が得られる。</p>
<p><span class="math display">$$
\vec{w} = \left(
\Phi^\top\Phi
\right)^{-1}
\Phi^\top \vec{t} \hspace{2mm}\cdots(5)
$$</span></p>
<p>ここで計画行列<span class="math inline"><em>Φ</em></span>は</p>
<p><span class="math display">$$
\Phi =
\begin{pmatrix}
\hspace{2mm}1 &amp; \phi_1(x_1) &amp; \cdots &amp; \phi_M(x_1)\\
\hspace{2mm}1 &amp; \phi_1(x_2) &amp; \cdots &amp; \phi_M(x_2)\\
 &amp; \cdots &amp; \\
\hspace{2mm}1 &amp; \phi_1(x_n) &amp; \cdots &amp; \phi_M(x_n)\\
\end{pmatrix} \hspace{2mm}\cdots(6)
$$</span> のように書ける。</p>
<p><strong>練習3:</strong> 式（5）を導け。</p>
<p>ここまでで、素晴らしいことに気づく。式(3)-(6)を見るとおり、この結果は関数の選び方やその数によらない形で得られているという事実である。関数に複雑な形をした関数を選べば、フィッティングも複雑なデータに対応でき、応用範囲が広がりそうである。にも関わらず、(2)のような多項式であろうともっと複雑な関数セットを選ぼうと(5)のような形で得られてしまうのは何故か？それは仮定したフィッティング関数式(3)が<strong>求めたいパラメータ<span class="math inline"><em>w⃗</em></span>に対して線形</strong>であるからである。 どんな関数を用意しても、その重み<span class="math inline"><em>w</em><sub><em>i</em></sub></span>の調整で線形和（重ね合わせ）で表現してしまえば、結論は同じになる。これはすごいことだ! もちろん、限界も現れる。それは和の数には限りがあることに起因したり、適切な関数の選択をどうすれば良いのかが職人芸になる、といった点である。 そのような課題にどのように立ち向かうのか、それは後半の講義で少しずつ明らかになろう。</p>
<h2 id="最小二乗法の別の見方最尤推定">最小二乗法の別の見方：最尤推定</h2>
<p>データフィッティングを、データ点とフィッティング関数の誤差の二乗が最小になるという観点でこれまで定式化してきた。 これを確率的な観点から眺め、最小二乗法の背景にある理論とその拡張に見通しをつけるために、同じ結論に至る最尤推定をここで学ぶ。</p>
<p>データフィッティングの目標は、<span class="math inline"><em>N</em></span>個のデータ<span class="math inline">(<em>x</em><sub><em>n</em></sub>, <em>t</em><sub><em>n</em></sub>), <em>n</em> = 1 ∼ <em>N</em></span>に基づいて、新たなデータ<span class="math inline"><em>x</em></span>に対する<span class="math inline"><em>t</em></span>を予測する、というように見ることができる。 目標とする<span class="math inline"><em>t</em></span>の予測の不確実性は、確率分布をつかって表すことができる。与えられた<span class="math inline"><em>x</em></span>に対する<span class="math inline"><em>t</em></span>は、平均がフィッティング関数<span class="math inline"><em>y</em></span>となるガウス関数に従うものと仮定する。 <span class="math display">$$
p(t|x, \vec{w}, \beta) = {\cal N}(t|y(x,\vec{w}),\beta^{-1}) \hspace{2mm}\cdots(7)
$$</span> ここで<span class="math inline">${\cal N}(t|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{(\frac{1}{2\sigma^2}(t-\mu)^2)}$</span> は変数が<span class="math inline"><em>t</em></span>, 平均が<span class="math inline"><em>μ</em></span>, 標準偏差が<span class="math inline"><em>σ</em></span>のガウス分布（正規分布）を表す。 訓練データ<span class="math inline">(<em>x</em><sub><em>n</em></sub>, <em>t</em><sub><em>n</em></sub>), <em>n</em> = 1 ∼ <em>N</em></span> = (<span class="math inline"><em>x⃗</em>, <em>t⃗</em></span>)を使って<span class="math inline"><em>w⃗</em></span>と<span class="math inline"><em>β</em></span>を求める方法に最尤推定がある。 これは、観測されたデータ (<span class="math inline"><em>x⃗</em>, <em>t⃗</em></span>)が得られる確率（尤度）が、それぞれの観測が独立の時、 <span class="math display">$$
p(\vec{t}|\vec{x}, \vec{w}, \beta) = \prod_{n=1}^{N}{\cal N}(t_n|y(x_n,\vec{w}),\beta^{-1}) \hspace{2mm}\cdots(8)
$$</span> となるので、この尤度を最大にする<span class="math inline"><em>w⃗</em></span>と<span class="math inline"><em>β</em></span>を求める、と考えることができるからである。 では、尤度の最大化を考えてみよう。その前に式()の対数をとる。これは対数は単調関数のため、最大値を与える変数<span class="math inline"><em>w⃗</em></span>と<span class="math inline"><em>β</em></span>の位置を変えないことを利用する。 対数をとると、 <span class="math display">$$
\ln p(\vec{t}|\vec{x}, \vec{w}, \beta) = -\frac{\beta}{2}\sum_{n=1}^N \left( y(x_n) - t_n \right)^2 + \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2\pi)
\hspace{2mm}\cdots(9)
$$</span> となる。この関数を<span class="math inline"><em>w⃗</em></span>の関数とみなした時、右辺の第二項と第三項は除いて良い（実際に偏微分すると0になる）。 式(4)と見比べると、前にかかる<span class="math inline"><em>β</em></span>と符号が異なるが、<span class="math inline"><em>β</em></span>は最大にする<span class="math inline"><em>w⃗</em></span>の値を変えず、 また、符号は最大化と最小化の違いであることから、 <strong>尤度関数式(8)を最大にする最尤推定の手法は、最小二乗法と全く同じフィッティング結果を与える</strong>。</p>
<p><strong>練習4:</strong> 式（9）を導け。 <br> <br> <br> <br> <br></p>
<h2 id="pythonを使った最小二乗法">pythonを使った最小二乗法</h2>
<h3 id="グラフ表示">グラフ表示</h3>
<p>CT-to-ED.csvをダウンロードし、グラフを表示させてみよう。 （参考）以下では、前回作成したinput_output.pyを編集して、 output.csvのデータをグラフ表示するスクリプトを作成します。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># データの読み込み</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;output.csv&quot;</span>, encoding<span class="op">=</span><span class="st">&quot;SHIFT-JIS&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2列目と3列目の和をとり、新たにtotalという項目名で4列目を作る</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;total&quot;</span>] <span class="op">=</span> df[<span class="st">&quot;sin_2px&quot;</span>]<span class="op">+</span>df[<span class="st">&quot;noise&quot;</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># データの出力（今回はコメントアウト）</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#df.to_csv(&quot;output2.csv&quot;, index=None, encoding=&quot;SHIFT-JIS&quot;)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 図の作成 \\</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.plot(df[<span class="st">&quot;x&quot;</span>], df[<span class="st">&quot;sin_2px&quot;</span>], ls<span class="op">=</span><span class="st">&quot;-&quot;</span>, color<span class="op">=</span><span class="st">&quot;red&quot;</span>, label<span class="op">=</span><span class="st">&quot;sin_2px&quot;</span>) <span class="co">#折れ線 </span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">&quot;x&quot;</span>], df[<span class="st">&quot;total&quot;</span>], marker<span class="op">=</span><span class="st">&quot;o&quot;</span>, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>,label<span class="op">=</span><span class="st">&quot;total&quot;</span>) <span class="co">#散布図 </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.0</span>) <span class="co"># xの範囲</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>) <span class="co"># yの範囲</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.legend() <span class="co"># レジェンドを追加</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout() <span class="co"># レイアウトの自動調整（常に書いておく）</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.show()  <span class="co"># 図の出力</span></span></code></pre></div>
<p><strong>練習１：</strong> 作成したスクリプトを編集し、 CT-to-ED.csvの１列目（CT_value_1）をx軸、 3列目（Physical_Density）をy軸にとった散布図と 2列目（CT_value_2）をx軸、 3列目（Physical_Density）をy軸にとった散布図を重ねたグラフを作成せよ。 <br> <br></p>
<h3 id="最小二乗法直線フィッティング">最小二乗法：直線フィッティング</h3>
<p>まずは、見た目で<span class="math inline"><em>y</em> = <em>a</em><em>x</em> + <em>b</em></span>を使って図のデータ点をなるべく通るような<span class="math inline"><em>a</em></span>と<span class="math inline"><em>b</em></span>を探してみよう。データを読み込んだ後に、下に示す部分をスクリプトに記載することで、直線を一緒にプロットすることができる。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="op">-</span><span class="dv">1000</span>, <span class="dv">1300</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="fl">0.2</span>,<span class="fl">1.7</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> (y[<span class="dv">1</span>]<span class="op">-</span>y[<span class="dv">0</span>])<span class="op">/</span>(x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> y[<span class="dv">1</span>]<span class="op">-</span>a<span class="op">*</span>x[<span class="dv">1</span>]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y=&quot;</span>,a,<span class="st">&quot;x+&quot;</span>,b)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, ls<span class="op">=</span><span class="st">&quot;-&quot;</span>, label<span class="op">=</span><span class="st">&quot;fitting&quot;</span>)</span></code></pre></div>
<p>（ここで何をやったのか、各自考えて理解すること）<br> 直線がちょっと合っていないようなので、上を修正して自分なりに合わせるように調整してみよう。</p>
<p>ここでやったことは、「見た目で合わせる」という作業である。 これは科学的な根拠に基づいた合わせ方ではないが、やってダメなことでは決してありません。 フィッティングの感触を掴む時にはむしろ直感的な方法を試してみるということは「有効な発想」です。一方、大学生なら最終的にはもっと明確で定量的な指標とともにフィッティングしてもらいたいものであるのも事実です。 最小二乗法を使った直線フィッティングは、フィッティング関数を<span class="math inline"><em>y</em> = <em>a</em><em>x</em> + <em>b</em></span>とした時に、 <u>データとの誤差<span class="math inline">${\cal L}(a,b)=\sum_n (y_n-t_n)^2$</span>を最小にする<span class="math inline"><em>a</em></span>と<span class="math inline"><em>b</em></span>を見つける</u>、という作業をすることでした。これを使うことで、明確で定量的なフィッティングができそうです。 この誤差の最小値を求める問題は、<span class="math inline"><em>a</em></span>と<span class="math inline"><em>b</em></span>で偏微分してそれが<span class="math inline"> = 0</span>となる時の<span class="math inline">(<em>a</em>, <em>b</em>)</span>を求めれば良い。では、そのようにして CT_valueを説明変数（<span class="math inline"><em>x</em></span>）としてPhysical_Density（<span class="math inline"><em>t</em></span>）をフィットするにはどうしたら良いのか？</p>
<p>今回はpythonが用意している数値計算用の「ライブラリ（library）」を利用して解いてみることにする（後の講義では、式(5)を自分で作成してもらうことも行います）。pythonはこれまでの歴史で積み上げられてきた様々な科学技術計算のライブラリを有しており、それを利用することで上記のような面倒な計算を簡便に行うメソッドを提供してくれている。最小二乗法のためのライブラリもいくつか用意されている； * Numpy の polyfit * Scipy の optimize.leastsq * Scipy の optimize.curve_fit</p>
<p>などなど。ここではNumpyのpolyfit（多項式フィット）を使ってフィッティングしてみよう。まずNumpyを使うため、それをimportする。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span></code></pre></div>
<p>（Numpyを使うためにはこれを予め宣言しておく必要がある）。</p>
<p>そして、以下をスクリプトの適切なところに書き込もう。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Least mean square fitting</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>coef <span class="op">=</span> np.polyfit(df[<span class="st">&quot;CT_value_1&quot;</span>],df[<span class="st">&quot;Physical_Density&quot;</span>],<span class="dv">1</span>) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> coef[<span class="dv">0</span>]<span class="op">*</span>df[<span class="st">&quot;CT_value_1&quot;</span>]<span class="op">+</span> coef[<span class="dv">1</span>] </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.plot(df[<span class="st">&quot;CT_value_1&quot;</span>], y_pred, ls<span class="op">=</span><span class="st">&quot;-&quot;</span>, color<span class="op">=</span><span class="st">&quot;black&quot;</span>,label<span class="op">=</span><span class="st">&quot;fitting_LMS&quot;</span>) </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;y=&quot;</span>,coef[<span class="dv">0</span>],<span class="st">&quot;x+&quot;</span>,coef[<span class="dv">1</span>]) </span></code></pre></div>
<p>ここで書かれた内容をスクリプトに追加することで、どのような出力が出るだろうか？１行ずつ何をしているかを自分の頭でしっかり考えること。みなさんなら、理解できるはず。</p>
<p><strong>練習２：</strong> np.polyfitを使った結果と目で合わせた結果をグラフ上で比較せよ。目で合わせたフィットと比べ、どちらが良い？ <br> <br> <br></p>
<h3 id="最小二乗法多項式フィッティング">最小二乗法：多項式フィッティング</h3>
<p>今度はoutput2.csvの１列目（x）と４列目（total）をフィットしてみよう。np.polyfitを早速使ってフィッティングしてみると…</p>
<p><strong>練習３：</strong> output2.csvの１列目（x）をx、 ４列目（total）をyとして、フィッティングせよ。 <br> <br> <br></p>
<p>いかがでしょう。フィットは成功しましたか？ うまくできない人は、以下をヒントに考えてみてよう。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Least mean square fitting</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>coef <span class="op">=</span> np.polyfit(df[<span class="st">&quot;CT_value_1&quot;</span>],df[<span class="st">&quot;Physical_Density&quot;</span>],<span class="dv">4</span>) </span></code></pre></div>
<p>ここで赤字で示した4は４次の多項式（<span class="math inline"><em>y</em> = <em>a</em> + <em>b</em><em>x</em> + <em>c</em><em>x</em><sup>2</sup> + <em>d</em><em>x</em><sup>3</sup> + <em>e</em><em>x</em><sup>4</sup></span>）を用いるということを意味する （前節ではここを１にしていたので直線近似していたことになる）。 ４次式をプロットしてみよう。まず多項式でフィットされた結果を出力する。その際、滑らかな曲線で表示したいので以下の作業により<span class="math inline"><em>x</em></span>のデータを細かくしておく。</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Least mean square fitting </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>coef <span class="op">=</span> np.polyfit(df[<span class="st">&quot;x&quot;</span>],df[<span class="st">&quot;total&quot;</span>],<span class="dv">4</span>) </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>) <span class="co">#表示するxを細かくする</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> coef[<span class="dv">0</span>]<span class="op">*</span>x<span class="op">*</span>x<span class="op">*</span>x<span class="op">*</span>x <span class="op">+</span> coef[<span class="dv">1</span>]<span class="op">*</span>x<span class="op">*</span>x<span class="op">*</span>x <span class="op">+</span> coef[<span class="dv">2</span>]<span class="op">*</span>x<span class="op">*</span>x <span class="op">+</span> coef[<span class="dv">3</span>]<span class="op">*</span>x <span class="op">+</span> coef[<span class="dv">4</span>] </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_pred, ls<span class="op">=</span><span class="st">&quot;-&quot;</span>, color<span class="op">=</span><span class="st">&quot;black&quot;</span>,label<span class="op">=</span><span class="st">&quot;fitting_LMS&quot;</span>)</span></code></pre></div>
<p>フィットできましたか？できた場合でも、疑問があればなぜできたのかをきちんと説明できるよう、疑問点を解消しましょう。</p>
<p>さて、多項式の数を変えるたびにy_predの形を次数に対応して変えなかればならないのは面倒だ。10次の多項式なんて、想像するだけで嫌ですね。np.polyfitに関して多項式の数に応じて計算してくれる関数funcを次のように作ることができる。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Least mean square fitting</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>) <span class="co">#表示するxを細かくする</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>func <span class="op">=</span> np.poly1d(np.polyfit(df[<span class="st">&quot;x&quot;</span>], df[<span class="st">&quot;total&quot;</span>], <span class="dv">4</span>)) <span class="co">#4次の多項式関数を作成</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, func(x), ls<span class="op">=</span><span class="st">&quot;-&quot;</span>, color<span class="op">=</span><span class="st">&quot;black&quot;</span>,label<span class="op">=</span><span class="st">&quot;fitting_LMS&quot;</span>) <span class="co">#func(x)でOK</span></span></code></pre></div>
<p>出力されましたね！np.poly1dを使うことで、np.polyfitでフィットして決まった係数<span class="math inline"><em>w</em></span>を使って任意の<span class="math inline"><em>x</em></span>で値を返す関数を作ることができています。</p>
<p>次に10次の多項式フィッティングをやってみよう。 結果はいかがでしたか？</p>
<!---
%問1：4次と10次の違いが生じた理由を考えよ。\\
%\vspace{1.5cm}
%df <- transform(data2, PredictedY = predict(poly.fit))
%plot(df$x,df$PredictedY)

%fm3=nls(y+rdat~a+b*x+c*x^2+d*x^3,start=c(a=1,b=1,c=1,d=1),trace=T) 
%plot(x,fitted(fm3),col=4)
%\section{分類}
--->
<p>今みなさんは、<u>過学習</u>という問題を目の当たりにしています。10次多項式のフィッティングで生じたこの問題の解決方法は、次週以降で探りたいと思います。</p>
<h2 id="演習レポート">演習レポート</h2>
<p>4次と10次の違いが生じた理由を考え、<u>解説</u>せよ。</p>
