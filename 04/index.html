<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<!--ここから追記 1.(数式に対応させる)-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax:{inlineMath: [['$', '$']]},
      messageStyle: "none"
    });
</script>
<!--追記 1.ここまで-->
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
    <!--ここから追記 2.(--- で改ページ)-->
    <style>  
      hr {  
        opacity: 0;  
        break-after: page;  
      }  
    </style>  
    <!--追記 2.ここまで-->
<h1 id="%E3%83%99%E3%82%A4%E3%82%BA%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0">ベイズ線形回帰</h1>
<h2 id="%E8%B6%85%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BFhyper-parameter">超パラメータ（hyper parameter）</h2>
<p>前回までに、最小二乗法（正則化最小二乗法）は、確率論的な枠組みとの対応の中で、最尤推定（事後分布最大化）に基づく一般化線形回帰とみなせるということがわかった。
簡単に言うと、事後分布$p(\vec{w}|\vec{t}, \vec{x}, \beta)$が、確率の乗法定理によって、
尤度関数$p(\vec{t}|\vec{x}, \vec{w}, \beta)$と事前分布$p(\vec{w}|\alpha)$の積に比例する</p>
<p>$$
p(\vec{w}|\vec{t}, \vec{x}, \beta) \propto p(\vec{t}|\vec{x}, \vec{w}, \beta) p(\vec{w})
\hspace{2mm}\cdots(1)
$$</p>
<p>という関係により、それを最大化するということが損失関数</p>
<p>$$
{\cal L}(\vec{w}) = \frac{\beta}{2}\sum_{n=1}^N \left( y(x_n) - t_n \right)^2 + \frac{\alpha}{2}|| \vec{w} ||^2 \hspace{2mm}\cdots(2)
$$</p>
<p>を最小化するということに一致するという事実を見てきた[*]。</p>
<p>*<em>データ予測や$\vec{w}$の事前分布がガウス分布に従うという仮定が入っている</em></p>
<p>両辺を$\beta$で割って正則化最小二乗法と比べると、$\lambda = \alpha/\beta$となり、
$\lambda$の値を適切な値に調整することで過学習が抑えられるということも、実際の演習を行うことで学んだ。</p>
<p>さて、式(2)の最小化で決まるのは$\vec{w}$であり、$\alpha,\beta$自体は決まらない。
どのように決めていたかというと、過学習が起こらないような結果になっているかを見て、
その時の$\alpha,\beta$の値（実際にはその比に対応する
$\lambda = \alpha/\beta$　）が適切であるということを正当化していた。手元にあるデータ全てを使ってフィッティングしているのであれば、それが過学習を引き起こしているかどうか一般的にはわからないことが多い。つまり、過学習が起こらないようにパラメータを決めるということは、答えを見て予測をしている（カンニングしている）ようなものと言え、もっと別な方法を本日の講義では考えてみたい。</p>
<p>ここで、事後分布最大化（損失関数の最小化）で決まらないパラメータを<strong>超パラメータ (hyper parameter)<strong>と呼ぶことにする。
そして、超パラメータの値を１つに選択することを</strong>モデル選択</strong>と呼び、これまで行ってきた一般化線形回帰において自由に値が取れる超パラメータ$\alpha,\beta$を、どのように決めるか（モデル選択するか）という問題に本章では踏み込んでいく。</p>
<p>ちなみに、モデル選択の問題は回帰だけでなく後半で学ぶ識別においても考えなければならない。しかし、以下に記すのと同じアプローチで識別問題も扱うことができることをここで付言しておく。</p>
<h2 id="%E3%83%A2%E3%83%87%E3%83%AB%E9%81%B8%E6%8A%9E%E3%81%AE%E6%96%B9%E6%B3%951%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E5%89%B2">モデル選択の方法1:データ分割</h2>
<p>モデル選択で最も簡単かつ便利な方法は、データを「訓練」「検証」「テスト」用に分割し、超パラメータを変えて訓練データで学習された$\vec{w}$を持つモデルを多数用意した中で、検証データを最もよく再現する超パラメータを選択する（モデル選択する）という方法である。テストデータは、モデル選択までの一連の作業では一切使わない。これは最終的に選択されたモデルの予測精度（or誤差）を評価する際に使用する。</p>
<p>データ分割の仕方にも様々な方法がある。代表的な方法としてk交差検証（k-fold cross validation）がある（図4-1）。
これは予めテスト用のデータを除いておき、残りのデータをkグループに分割後、それぞれのグループを順次検証用に使う（残りは訓練に使う）ということを行う。この極端な例がLOO法（Leave-one-out method）であり、一個だけ検証用として使う（残りは訓練）。
LOOはとりわけデータ数が限られている場合によく使われる方法である（訓練にはなるべく沢山のデータを使いたいのである）。</p>
<p><img src="images/4_1.png" width="500"><br>
<em>図4-1. データ分割の例: 5-fold cross validationでは、訓練と検証を合わせたデータを５つのグループにわけ、そのうちの１つを検証、残りを訓練に使用する。</em></p>
<p>グループにきちんと分けてしまうのではなく、データからランダムに検証用データをサンプリングする（残りは訓練データなので訓練データをサンプルする、という理解で良い）
という方法も考えられる。この場合はk交差検証と異なり訓練と検証のデータが重複しても良い（復元抽出）ようにもできる。これをブートストラップ法と呼んでいる。</p>
<p>事後分布最大化（正則化最小二乗法）に当てはめて考えてみよう。
まず、データから一部テスト用に取り出しておく。残りのデータからk交差検証やブートストラップ法によって訓練用と検証用にデータを分割し、抽出した訓練データで様々な$\lambda$に対してフィットされた$\vec{w}$を求める。検証データに当てはめた時、最も誤差が少ない$\lambda$（それに対応する最適解$\vec{w}$）を最適なモデルとして選ぶ。k交差検証やブートストラップ法ではこれが繰り返され、$\lambda$の平均値を最終的に選択されたモデルとみなす。</p>
<h2 id="%E3%83%A2%E3%83%87%E3%83%AB%E9%81%B8%E6%8A%9E%E3%81%AE%E6%96%B9%E6%B3%952%E3%83%99%E3%82%A4%E3%82%BA%E3%81%AE%E3%82%A2%E3%83%97%E3%83%AD%E3%83%BC%E3%83%81">モデル選択の方法2:ベイズのアプローチ</h2>
<p>データ分割をせずに超パラメータを求める方法もある。これはベイズ線形回帰と呼ばれる方法で、データを無駄にせずにモデル作成にFullに使えるというメリットがある。ベイズ線形回帰は、$\vec{w}$の事後分布が得られていることが前提となる。
我々はまだ「事後分布を最大にする$\vec{w}$」しか求めていないので[*]、まずは事後分布である式(1)を求めてみよう。
ここで、事前分布と尤度関数が</p>
<p>$$
p(\vec{x}) = {\cal N}(\vec{x}|\vec{\mu}, \Lambda^{-1}) \hspace{2mm}\cdots(3)\
p(\vec{y}|\vec{x}) = {\cal N}(\vec{y}|A\vec{x}+\vec{b}, L^{-1}) \hspace{2mm}\cdots(4)
$$</p>
<p>のようにガウス分布で書ける時、事後分布もまた</p>
<p>$$
p(\vec{x}|\vec{y}) = {\cal N}(\vec{x}|\Sigma(A^\top L(\vec{y}-\vec{b})), \Sigma), \hspace{2mm}where\hspace{1mm}\Sigma = (\Lambda + A^\top L A)^{-1}
\hspace{2mm}\cdots(5)
$$</p>
<p>とガウス分布になる。これを使って03「線形回帰モデルと正則化」のところで与えている線形回帰における尤度関数と事前分布、</p>
<p>$$
p(\vec{t}|\vec{x}, \vec{w}, \beta) = \prod_{n=1}^{N}{\cal N}(t_n|y(x_n,\vec{w}),\beta^{-1})
\hspace{2mm}\cdots(6)
$$</p>
<p>$$
p(\vec{w}|\alpha) = \left( \frac{\alpha}{\sqrt{2\pi}} \right)^{M/2}
\exp{(-\frac{\alpha}{2}\vec{w}^\top \vec{w})}
\hspace{2mm}\cdots(7)
$$</p>
<p>を当てはめると</p>
<p>$$
p_{prior}(\vec{w}|\alpha) = {\cal N}(\vec{w}|\vec{m}_N, \vec{S}_N)
\hspace{2mm}\cdots(8)
$$</p>
<p>$$
\vec{m}_N = \beta \vec{S}_N \Phi^\top \vec{t}
\hspace{2mm}\cdots(9)
$$</p>
<p>$$
\vec{S}_N^{-1} = \alpha I + \beta \Phi^\top \Phi
\hspace{2mm}\cdots(10)
$$</p>
<p>となる。ここで$\Phi$は、これも「３線形回帰モデルと正則化」で与えている計画行列である。
（面倒な計算が入るのでここでは結果だけを示すが、皆さんであれば応用数学などで既に習っている代数を使うことでこのように書けることを証明できると思います。）</p>
<p><strong>練習１：</strong>
式(8)を証明せよ。<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br></p>
<p>事後分布の具体的な形状がわかったので、ベイズ的な扱いに進んでいこう。ここでわかることは、今や求めたかった基底関数の重み付けを決定する$\vec{w}$はもはや１つの値ではなく、
<strong>幅を持った分布となっている</strong>という事実である。
事後分布最大化では、その分布で最も確からしいもの（点推定）だけを得て、それを使って予測（フィッティング）していた。しかしながら、今得られた結果は、確率は低くても、$\vec{w}$は点推定以外の値も取り得る、ということを示している。そうであるならば、分布を全て考慮して予測（フィッティング）してはどうか？これがベイズ線形回帰のいうところなのである。</p>
<p>$p_{prior}(\vec{w}|\alpha)$の確率とともに$t$を予測する（フィッティングする）というのは、$\vec{w}$は今や連続的なガウス分布になっているので、全ての取り得る可能な$\vec{w}$に対して
$p_{prior}(\vec{w}|\alpha)$という重みをつけた
積分、</p>
<p>$$
p(\vec{t}|\alpha, \beta) = \int p(\vec{t}|\vec{x}, \vec{w},\beta)p_{prior}(\vec{w}|\alpha)d\vec{w}
\hspace{2mm}\cdots(11)
$$</p>
<p>を実行するということである。この積分は解析的に解けないので、簡単のため、被積分関数で一番値が大きいところを一致させたガウス分布に近似し積分する（エビデンス近似という）。
すると、</p>
<p>$$
p(\vec{t}|\alpha, \beta) \sim \left(\frac{\beta}{2\pi}\right)^{N/2}
\left(\frac{\alpha}{2\pi}\right)^{M/2}
\exp{(-E(\vec{m}_N)} (2\pi)^{M/2} |S_N^{-1}|^{-1/2},
\hspace{2mm}\cdots(12)
$$
ここで、
$$
E(\vec{w}) = \frac{\beta}{2}||\vec{t}-\vec{\phi}^\top\vec{w}||^2+\frac{\alpha}{2}\vec{w}^\top\vec{w}
$$</p>
<p>となり、さらに対数をとると、</p>
<p>$$
\ln p(\vec{t}|\alpha, \beta) \sim
\frac{M}{2}\ln \alpha
+\frac{N}{2}\ln \beta
-E(\vec{m}_N) (2\pi)^{M/2}
-\frac{1}{2}\ln|S_N^{-1}|
-\frac{N}{2}\ln(2\pi)
\hspace{2mm}\cdots(13)
$$</p>
<p>となる。
手持ちのデータにおいて、これを最大にする$\alpha$と$\beta$を求めることで、先に見たような訓練用データと検証用データを分けることなしにモデル選択を行うことができる。ただし、陽な形では求まらないので、繰り返し演算（逐次演算）を行う；最初に適当な$\alpha$を与えて</p>
<p>$$
\gamma = \sum_i^M \frac{\lambda_i}{\alpha+\lambda_i}
\hspace{2mm}\cdots(14)
$$</p>
<p>を計算し、$\beta$も適当に与えて$\lambda=\alpha/\beta$を使ってその正則化のもとで$\vec{w}$を最適化する。今度はその最適化された$\vec{w}_{opt}=\vec{w}$を使って、</p>
<p>$$
\alpha =
\frac{\gamma}
{\vec{w}^\top \vec{w}}
\hspace{2mm}\cdots(15)
$$</p>
<p>のように$\alpha$を更新する。$\beta$も</p>
<p>$$
\frac{1}{\beta} = \frac{1}{N-\gamma}\sum_n^N(\vec{w}_{opt}^\top\vec{\phi}(x_n)-t_n)^2
\hspace{2mm}\cdots(16)
$$</p>
<p>で更新する。こうして$\alpha$、$\beta$、$\vec{w}$が同じデータから最適解が得られる。</p>
<p><strong>練習2:</strong></p>
<ul>
<li>(1) k-fold cross validationに対するエビデンス近似によるモデル選択のメリット・デメリットを述べよ。</li>
<li>(2) エビデンス近似における$\gamma$は有効パラメータ数と呼ばれる。それは何故か？（ヒント：<strong>パラメータ</strong>の意味しているところを明確にしましょう）</li>
</ul>
<h2 id="python%E3%81%AB%E3%82%88%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E9%81%B8%E6%8A%9E">pythonによるモデル選択</h2>
<h3 id="%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E5%89%B2%E3%81%AB%E3%82%88%E3%82%8B%E6%96%B9%E6%B3%95">データ分割による方法</h3>
<p>regression_K-fold.pyを使う。
このスクリプトはgauss_direct_and_numerical.pyをベースに作られており、03（「線形回帰モデルと正則化」）の資料も見ながら理解を進めていこう。</p>
<p>main関数（70行目）までのところ（1行目~69行目）は、使用するライブラリや関数の定義が書かれている。</p>
<p>まずk_basisで使用する基底関数（gaussかpolynomial）を指定する。続いてデータを入力する。k-fold.csvを用意しているので、それをダウンロードして読み込むようにする。そのデータをプロットしてみよう。これは全部で100点のデータである。</p>
<p>次にデータ分割を行う。</p>
<pre class="hljs"><code><div><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(kf):
    df_valid = df_all[k::kf]  <span class="hljs-comment"># k-fold</span>
    <span class="hljs-comment">#df_valid = df.sample(kf) # random sampling ランダムにサンプルする場合、こちらを使用</span>
    df = df_all.drop(df_valid.index) <span class="hljs-comment">#df_valid以外の残りをdfとする</span>
</div></code></pre>
<p>kfは分割するグループの数で、defaultでは5に設定してある。つまり100点のデータでは訓練に80個、検証に20個のデータを使う。df_all[k::kf]はスタート行kからkf置きに行を取り出すという意味である。これによってどのようにdf_allのデータがdf_validとdfに振り分けられるか、printで出力して確認しておくこと。</p>
<p>基底関数の設定（今回は10次の多項式を使う）し、最適化を行う。
分割したデータのうち、dfを使って$\vec{w}$を最適化するが、
その際に超パラメータlambの値を指定しなければならない。
ここでは、lambを初期値1として0.8を繰り返し掛けて値を小さくしていく中で、フィットされたモデルがdf_validにあるデータを最もよく再現するところを見つけてみよう[*]</p>
<p><em>このように、ある数値の範囲を適度な間隔で網羅的にサーチする手法をグリッドサーチ（grid search）という。</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">for</span> L <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):
    w = np.zeros(num_basis)
    lamb *= <span class="hljs-number">0.8</span>
    direct_w = direct_weight_optimize(y_true,basis,lamb)
</div></code></pre>
<p>これで順次変更したlambの値に応じて最適化された$\vec{w}$がdirect_wに格納される。</p>
<p><strong>練習１：</strong>
サーチするlambの範囲を求めよ。何回0.8倍する？
<br>
<br>
<br></p>
<p>次に5つの検証データでフィットされたモデルの精度を確認する。</p>
<pre class="hljs"><code><div>x = df_valid[<span class="hljs-string">"x"</span>] 
y = df_valid[<span class="hljs-string">"total"</span>]
<span class="hljs-keyword">if</span> k_basis == <span class="hljs-string">"gauss"</span>:
    basis_valid = gauss_basis_set_calc(num_basis,x,mu,sigma)
<span class="hljs-keyword">elif</span> k_basis == <span class="hljs-string">"polynomial"</span>:
    basis_valid = basis_set_calc(num_basis,x)
fitted = np.dot(direct_w,basis_valid)
mse_value = mean_squared_error(fitted, y)
<span class="hljs-keyword">if</span> mse_value &lt; min_mse:
    min_lamb = lamb
    min_mse = mse_value
</div></code></pre>
<p>min_mseに最小となる二乗和誤差、min_lambにその時のlambの値が格納される。5分割検証によって得られる5つのlambの値は
その次の行にあるdata_min_lambに入れられ、その平均値を使って全体のデータで最終的なモデルを作成する（151, 152行）。</p>
<p>この最終モデルを、潜在的な答えであるsin関数と比べて性能を測ってみよう（一般化できるかどうか？汎用性があるか？$\rightarrow$汎化性能を調べる）。156行目以下は、モデルの関数とsin関数の同時プロットと、図の中にsin関数とのRMSEを示すためのコードである。</p>
<p><strong>練習２：</strong>
10交差検証を実施し、5交差検証の結果と比較せよ。
<br>
<br>
<br>
<br></p>
<h3 id="%E3%82%A8%E3%83%93%E3%83%87%E3%83%B3%E3%82%B9%E8%BF%91%E4%BC%BC%E3%81%AB%E3%82%88%E3%82%8B%E6%96%B9%E6%B3%95">エビデンス近似による方法</h3>
<p>次にエビデンス近似によって超パラメータを決定し、そのモデルによるデータフィッティングの汎化性能を求めて上の結果と比較してみよう。
regression_Evidence.pyを用いる。またデータも同じk-fold.csvである。
使用する基底関数も同じ10次の多項式とする。
111行目まではほとんど同じ。次に計画行列の積（$\Phi^{\top}\Phi$）の固有値を求める。</p>
<pre class="hljs"><code><div><span class="hljs-comment"># eigen value 固有値を求める</span>
eig , eig_vec = LA.eig(np.dot(basis , basis.T))  
<span class="hljs-comment"># initial value</span>
alpha = <span class="hljs-number">0.0001</span>
beta = <span class="hljs-number">1</span>
<span class="hljs-comment"># max num. of iteration</span>
num = <span class="hljs-number">20</span>
</div></code></pre>
<p>alphaとbetaの初期値を与え、繰り返しながらデータにフィットするように更新する（ここでは20回更新）。
式(15)-(16)を20回繰り返す。</p>
<pre class="hljs"><code><div><span class="hljs-keyword">for</span> ite <span class="hljs-keyword">in</span> range(num):
    gam = np.sum(eig/(alpha/beta+eig))
    direct_w = direct_weight_optimize(y_true,basis,alpha/beta)
    mTm = np.dot(direct_w.T , direct_w)
    alpha = gam / mTm
    fitted = np.dot(direct_w,basis)
    beta = (len(y_true)-gam)
    mean_squared_error(fitted, y_true)
    print(alpha, beta, gam, mTm)
</div></code></pre>
<p>最後にsin関数と比べて性能を測る。</p>
<p>$\vec{w}$の予測分布は確率的であるため、よって$\vec{w}$がある範囲で分布している。そのため、$t$の予測値も確率的となる。つまり、$t$の予測範囲も標準誤差の範囲で描かせることができる。そのプロットも表示される（説明略）。</p>
<h2 id="%E6%BC%94%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88">演習レポート</h2>
<ul>
<li>(1) k-fold.csvのデータに対してのデータ分割の方法とエビデンス近似の方法によるモデル選択の結果を比べよ。</li>
<li>(2) 両者が一致しないのはなぜか？そもそも一致する条件はあるか？</li>
<li>(3) どちらが優れた方法か？条件によってデータ分割またはエビデンス近似の方が優れている、というように変わることがあるか？</li>
</ul>

</body>
</html>
